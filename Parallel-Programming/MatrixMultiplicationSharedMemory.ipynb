{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2TBfftb9lun",
        "outputId": "7e33d41b-4ccc-4d91-82cd-5bdf0969b57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin    cuda\tcuda-11.8  games\t       include\tlib64\t   man\t share\n",
            "colab  cuda-11\tetc\t   _gcs_config_ops.so  lib\tlicensing  sbin  src\n"
          ]
        }
      ],
      "source": [
        "! ls /usr/local/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version # nvcc compiler version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxJeoeyZ9uZg",
        "outputId": "fd650783-6a17-4ea6-bca2-a5d6cd4b8348"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MatrixMultiplication.cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<cuda_runtime.h>\n",
        "\n",
        "/* Variables residing in shared memory can be accessed at very high speed on a higly parallel manner;\n",
        "all threads in the same block have access to shared memory */\n",
        "\n",
        "#define BlockSize 32\n",
        "\n",
        "// Kernel Function\n",
        "__global__ void MatrixMultiplication(float *A, float *B, float *C, int N, int K, int M){\n",
        "  __shared__ float As[BlockSize][BlockSize]; // shared memory for matrix A\n",
        "  __shared__ float Bs[BlockSize][BlockSize]; // shared memory for matrix B\n",
        "\n",
        "  int bx = blockIdx.x; // blocks indices\n",
        "  int by = blockIdx.y;\n",
        "\n",
        "  int tx = threadIdx.x; // threads indices\n",
        "  int ty = threadIdx.y;\n",
        "\n",
        "  int ABegin = K * BlockSize * by;\n",
        "  int AEnd = ABegin + K - 1;\n",
        "  int AStep = BlockSize;\n",
        "\n",
        "  int BBegin = BlockSize * bx;\n",
        "  int BStep = BlockSize * M;\n",
        "\n",
        "  float Sum = 0.0f;\n",
        "\n",
        "  for(int a = ABegin, b = BBegin; a <= AEnd; a += AStep, b += BStep){\n",
        "    As[ty][tx] = A[a + K * ty + tx];\n",
        "    Bs[ty][tx] = B[b + M * ty + tx];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    #pragma unroll\n",
        "    for(int i = 0; i < BlockSize; i++){\n",
        "      Sum += As[ty][i] * Bs[i][tx];\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  int c = M * BlockSize * by + BlockSize * bx;\n",
        "  C[c + M * ty + tx] = Sum;\n",
        "}\n",
        "\n",
        "// Main code executed by the host\n",
        "int main(void){\n",
        "\n",
        "  int N = 12; // rows matrix A\n",
        "  int K = 14; // columns matrix A; rows matrix B\n",
        "  int M = 16; // columns matrix B\n",
        "\n",
        "  float *Ah, *Bh, *Ch; // host matrix pointers\n",
        "  Ah = (float *)malloc(N*K*sizeof(float)); // allocate host memory\n",
        "  Bh = (float *)malloc(K*M*sizeof(float));\n",
        "  Ch = (float *)malloc(N*M*sizeof(float));\n",
        "\n",
        "  for(int i = 0; i < N; i++){ // initialize elements matrix A\n",
        "    for(int j = 0; j < K; j++){\n",
        "      Ah[i*K+j] = i + 1.0;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  for(int i = 0; i < K; i++){ // initialize elements matrix B\n",
        "    for(int j = 0; j < M; j++){\n",
        "      Bh[i*M+j] = (i + 1.0)*2;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"\\n Matrix A. \\n\");\n",
        "  for(int i = 0; i < N; i++){\n",
        "    for(int j = 0; j < K; j++){\n",
        "      printf(\"%.2lf \", Ah[i*K+j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  printf(\"\\n Matrix B. \\n\");\n",
        "  for(int i = 0; i < K; i++){\n",
        "    for(int j = 0; j < M; j++){\n",
        "      printf(\"%.2lf \", Bh[i*M+j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  float *Ad, *Bd, *Cd;\n",
        "  cudaMalloc((void **)&Ad, sizeof(float) * N * K); // allocate GPU memory\n",
        "  cudaMalloc((void **)&Bd, sizeof(float) * K * M);\n",
        "  cudaMalloc((void **)&Cd, sizeof(float) * N * M);\n",
        "\n",
        "  cudaMemcpy(Ad, Ah, sizeof(float) * N * K, cudaMemcpyHostToDevice); // copy data from host to device (matrix A)\n",
        "  cudaMemcpy(Bd, Bh, sizeof(float) * K * M, cudaMemcpyHostToDevice); // copy data from host to device (matrix B)\n",
        "\n",
        "  dim3 blockSize(BlockSize, BlockSize); // threads per block\n",
        "  dim3 GridSize((M - 1) / blockSize.x + 1, (N - 1) / blockSize.y + 1); // no. of blocks\n",
        "\n",
        "  MatrixMultiplication<<<GridSize, blockSize>>>(Ad, Bd, Cd, N, K, M); // kernel function calling\n",
        "\n",
        "  cudaMemcpy(Ch, Cd, sizeof(float) * N * M, cudaMemcpyDeviceToHost); // copy data from device back to host (matrix C)\n",
        "\n",
        "  printf(\"\\n Matrix C. \\n\");\n",
        "  for(int i = 0; i < N; i++){\n",
        "    for(int j = 0; j < M; j++){\n",
        "      printf(\"%.2lf \", Ch[i*M+j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  free(Ah); // free host memory\n",
        "  free(Bh);\n",
        "  free(Ch);\n",
        "\n",
        "  cudaFree(Ad); // free GPU memory\n",
        "  cudaFree(Bd);\n",
        "  cudaFree(Cd);\n",
        "\n",
        "  return(0);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vy_OJvC9yzx",
        "outputId": "636ab876-fe71-48ee-9425-a58e1346d2b8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting MatrixMultiplication.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc MatrixMultiplication.cu -o test"
      ],
      "metadata": {
        "id": "-Umv-foJ9vx-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qe6Aoe09yLm",
        "outputId": "f57db60c-7f2d-46da-d6b5-a4dc7098555c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Matrix A. \n",
            "1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 \n",
            "2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 \n",
            "5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 \n",
            "6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 \n",
            "7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 \n",
            "8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 \n",
            "9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 \n",
            "10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 \n",
            "11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 \n",
            "12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 \n",
            "\n",
            " Matrix B. \n",
            "2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 \n",
            "4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 \n",
            "6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 \n",
            "8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 \n",
            "10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 \n",
            "12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 \n",
            "14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 \n",
            "16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 \n",
            "18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 \n",
            "20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 \n",
            "22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 \n",
            "24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 \n",
            "26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 \n",
            "28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 \n",
            "\n",
            " Matrix C. \n",
            "210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 \n",
            "420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 \n",
            "630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 \n",
            "840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 \n",
            "832.00 832.00 832.00 832.00 832.00 832.00 832.00 832.00 832.00 832.00 832.00 832.00 832.00 832.00 832.00 832.00 \n",
            "1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 \n",
            "1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 \n",
            "1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 \n",
            "1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 \n",
            "2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 \n",
            "2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 2080.00 \n",
            "2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 \n"
          ]
        }
      ]
    }
  ]
}