{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFUw6tf3kejA",
        "outputId": "f546a7bc-34a2-45bf-88e4-791fd2052c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin    cuda\tcuda-11.8  games\t       include\tlib64\t   man\t share\n",
            "colab  cuda-11\tetc\t   _gcs_config_ops.so  lib\tlicensing  sbin  src\n"
          ]
        }
      ],
      "source": [
        "! ls /usr/local/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version # nvcc compiler version"
      ],
      "metadata": {
        "id": "iZcTj1EUk48f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a10988-6c04-4231-d336-aff4e90b41c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MatrixMultiplication.cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<cuda_runtime.h>\n",
        "\n",
        "/* By default, memory on the device is global memory\n",
        "global memory plays the same role as the Random Access Memory on a CPU */\n",
        "\n",
        "// Kernel Function\n",
        "__global__ void MatrixMultiplication(float *A, float *B, float *C, int N, int K, int M){\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x; // (block index x-axis * block dimension x-axis) + thread index x-axis (within the block) == column index\n",
        "  int idy = blockIdx.y * blockDim.y + threadIdx.y; // (block index y-axis * block dimension y-axis) + thread index y-axis (within the block) == row index\n",
        "\n",
        "  if(idy < N && idx < M){\n",
        "    float sum = 0.0;\n",
        "    for(int i = 0; i < K; i++){\n",
        "      sum += A[idy * K + i]*B[i * M + idx];\n",
        "    }\n",
        "    C[idy * M + idx] = sum;\n",
        "  }\n",
        "}\n",
        "\n",
        "// Main code executed by the host\n",
        "int main(void){\n",
        "  int N = 12; // rows matrix A\n",
        "  int K = 14; // columns matrix A; rows matrix B\n",
        "  int M = 16; // columns matrix B\n",
        "\n",
        "  float *Ah, *Bh, *Ch; // host matrix pointers\n",
        "  Ah = (float *)malloc(N*K*sizeof(float)); // allocate host memory\n",
        "  Bh = (float *)malloc(K*M*sizeof(float));\n",
        "  Ch = (float *)malloc(N*M*sizeof(float));\n",
        "\n",
        "  for(int i = 0; i < N; i++){ // initialize elements matrix A\n",
        "    for(int j = 0; j < K; j++){\n",
        "      Ah[i*K+j] = i + 1.0;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  for(int i = 0; i < K; i++){ // initialize elements matrix B\n",
        "    for(int j = 0; j < M; j++){\n",
        "      Bh[i*M+j] = (i + 1.0)*2;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"\\n Matrix A. \\n\");\n",
        "  for(int i = 0; i < N; i++){\n",
        "    for(int j = 0; j < K; j++){\n",
        "      printf(\"%.2lf \", Ah[i*K+j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  printf(\"\\n Matrix B. \\n\");\n",
        "  for(int i = 0; i < K; i++){\n",
        "    for(int j = 0; j < M; j++){\n",
        "      printf(\"%.2lf \", Bh[i*M+j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  float *Ad, *Bd, *Cd;\n",
        "  cudaMalloc((void **)&Ad, sizeof(float) * N * K); // allocate GPU memory\n",
        "  cudaMalloc((void **)&Bd, sizeof(float) * K * M);\n",
        "  cudaMalloc((void **)&Cd, sizeof(float) * N * M);\n",
        "\n",
        "  cudaMemcpy(Ad, Ah, sizeof(float) * N * K, cudaMemcpyHostToDevice); // copy data from host to device (matrix A)\n",
        "  cudaMemcpy(Bd, Bh, sizeof(float) * K * M, cudaMemcpyHostToDevice); // copy data from host to device (matrix B)\n",
        "\n",
        "  dim3 BlockSize(16, 16); // threads per block\n",
        "  dim3  GridSize((M + BlockSize.x - 1) / BlockSize.x, (N + BlockSize.y - 1) / BlockSize.y); // no. of blocks\n",
        "\n",
        "  MatrixMultiplication<<<GridSize, BlockSize>>>(Ad, Bd, Cd, N, K, M); // kernel function calling\n",
        "\n",
        "  cudaMemcpy(Ch, Cd, sizeof(float) * N * M, cudaMemcpyDeviceToHost); // copy data from device back to host (matrix C)\n",
        "\n",
        "  printf(\"\\n Matrix C. \\n\");\n",
        "  for(int i = 0; i < N; i++){\n",
        "    for(int j = 0; j < M; j++){\n",
        "      printf(\"%.2lf \", Ch[i*M+j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "\n",
        "  free(Ah); // free host memory\n",
        "  free(Bh);\n",
        "  free(Ch);\n",
        "\n",
        "  cudaFree(Ad); // free GPU memory\n",
        "  cudaFree(Bd);\n",
        "  cudaFree(Cd);\n",
        "\n",
        "  return(0);\n",
        "}"
      ],
      "metadata": {
        "id": "uFNB9NJak7Lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c148da-9702-4542-de59-be5a6858e050"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing MatrixMultiplication.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc MatrixMultiplication.cu -o test"
      ],
      "metadata": {
        "id": "3hnglV9Bk7S6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./test"
      ],
      "metadata": {
        "id": "DP-0SQWWk7aJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4102004-6193-4aac-dbdf-23f36ff8836d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Matrix A. \n",
            "1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 \n",
            "2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 \n",
            "3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 \n",
            "4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 \n",
            "5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 5.00 \n",
            "6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 \n",
            "7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 7.00 \n",
            "8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 \n",
            "9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 9.00 \n",
            "10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 \n",
            "11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 11.00 \n",
            "12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 \n",
            "\n",
            " Matrix B. \n",
            "2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 2.00 \n",
            "4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 4.00 \n",
            "6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 6.00 \n",
            "8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 8.00 \n",
            "10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 \n",
            "12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 12.00 \n",
            "14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 14.00 \n",
            "16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 16.00 \n",
            "18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 18.00 \n",
            "20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 20.00 \n",
            "22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 22.00 \n",
            "24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 24.00 \n",
            "26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 26.00 \n",
            "28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 28.00 \n",
            "\n",
            " Matrix C. \n",
            "210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 210.00 \n",
            "420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 420.00 \n",
            "630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 630.00 \n",
            "840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 840.00 \n",
            "1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 1050.00 \n",
            "1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 1260.00 \n",
            "1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 1470.00 \n",
            "1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 1680.00 \n",
            "1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 1890.00 \n",
            "2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 2100.00 \n",
            "2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 2310.00 \n",
            "2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 2520.00 \n"
          ]
        }
      ]
    }
  ]
}