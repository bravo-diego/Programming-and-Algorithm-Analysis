{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIkGvMglTRhT",
        "outputId": "1b2b95d9-fdf2-454f-bde3-55549ae37c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin    cuda\tcuda-11.8  games\t       include\tlib64\t   man\t share\n",
            "colab  cuda-11\tetc\t   _gcs_config_ops.so  lib\tlicensing  sbin  src\n"
          ]
        }
      ],
      "source": [
        "! ls /usr/local/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pSfQ59STUF3",
        "outputId": "04372bda-481c-4ace-b387-648e06fc5b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "! nvcc --version # nvcc compiler version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile VectorAddition.cu\n",
        "#include<stdio.h>\n",
        "#include<time.h>\n",
        "#include<stdlib.h>\n",
        "#include<cuda_runtime.h>\n",
        "\n",
        "// Kernel Function - GPU function that is meant to be called from CPU code\n",
        "__global__ void VectorAddition(float *c_d, float *a_d, float *b_d, int N){\n",
        " int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        " if (idx<N){\n",
        "  c_d[idx] = a_d[idx] + b_d[idx];\n",
        " }\n",
        "}\n",
        "\n",
        "// Main code executed by the host\n",
        "int main(void){\n",
        "  float *a_h, *b_h, *c_h; // pointers host\n",
        "  float *a_d, *b_d, *c_d; // pointers device\n",
        "  const int N = 24; // No. of elements in arrays\n",
        "\n",
        "  size_t size=N * sizeof(float);\n",
        "\n",
        "  a_h = (float *)malloc(size);\n",
        "  b_h = (float *)malloc(size);\n",
        "  c_h = (float *)malloc(size);\n",
        "\n",
        "  srand(time(NULL));\n",
        "  for(int i = 0; i < N; i++){\n",
        "    a_h[i] = rand() % 100 + 1.0;\n",
        "    b_h[i] = rand() % 100 + 1.0;\n",
        "  }\n",
        "\n",
        "  printf(\"\\nArray a:\\n\");\n",
        "  for(int i = 0; i < N; i++) printf(\"%f \", a_h[i]);\n",
        "  printf(\"\\nArray b:\\n\");\n",
        "  for(int i = 0; i < N; i++) printf(\"%f \", b_h[i]);\n",
        "\n",
        "  cudaMalloc((void **) &a_d, size);\n",
        "  cudaMalloc((void **) &b_d, size);\n",
        "  cudaMalloc((void **) &c_d, size);\n",
        "\n",
        "  cudaMemcpy(a_d, a_h, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(b_d, b_h, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  int block_size = 8;\n",
        "  int n_blocks = N/block_size + (N%block_size == 0 ? 0:1);\n",
        "\n",
        "  VectorAddition<<<n_blocks, block_size>>>(c_d, a_d, b_d, N);\n",
        "\n",
        "  cudaMemcpy(c_h, c_d, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  printf(\"\\nArray c:\\n\");\n",
        "  for(int i = 0; i < N; i++) printf(\"%f \", c_h[i]);\n",
        "\n",
        "  free(a_h);\n",
        "  free(b_h);\n",
        "  free(c_h);\n",
        "\n",
        "  cudaFree(a_d);\n",
        "  cudaFree(b_d);\n",
        "  cudaFree(c_d);\n",
        "\n",
        "  return(0);\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOo2ffh2UJXK",
        "outputId": "77c91011-bbb5-446f-903c-c7ca92012add"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting VectorAddition.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc VectorAddition.cu -o test"
      ],
      "metadata": {
        "id": "Ivzdi--2X2R_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIkJY9cIX6iJ",
        "outputId": "b1c55262-a73a-44cd-ffc3-62f460cc9c57"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Array a:\n",
            "28.000000 77.000000 98.000000 93.000000 94.000000 42.000000 77.000000 46.000000 70.000000 70.000000 55.000000 54.000000 26.000000 52.000000 12.000000 30.000000 4.000000 85.000000 25.000000 30.000000 39.000000 66.000000 94.000000 81.000000 \n",
            "Array b:\n",
            "81.000000 95.000000 68.000000 77.000000 10.000000 96.000000 24.000000 14.000000 25.000000 93.000000 36.000000 87.000000 16.000000 58.000000 72.000000 91.000000 6.000000 1.000000 30.000000 70.000000 71.000000 16.000000 11.000000 15.000000 \n",
            "Array c:\n",
            "109.000000 172.000000 166.000000 170.000000 104.000000 138.000000 101.000000 60.000000 95.000000 163.000000 91.000000 141.000000 42.000000 110.000000 84.000000 121.000000 10.000000 86.000000 55.000000 100.000000 110.000000 82.000000 105.000000 96.000000 "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}